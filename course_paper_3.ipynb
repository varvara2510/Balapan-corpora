{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M3dJvCYGFQb"
      },
      "source": [
        "# I этап: подготовка текстов для НКРЯ и Balapan corpora (сайта)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLzW4SSwOMVc"
      },
      "source": [
        "## 0. Предочистка файла"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def preprocess_xml(input_file):\n",
        "    output_file = input_file.replace('.xml', '_precleaned.xml')\n",
        "\n",
        "    tree = ET.parse(input_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for para in root.findall('.//para'):\n",
        "        for text in para.findall('se'):\n",
        "            if text.text is not None and '~~~' in text.text:\n",
        "                text.text = re.sub(r'~~~', '', text.text)\n",
        "            elif text.text is not None and re.findall(r'\\[\\d+\\]', text.text):\n",
        "                text.text = re.sub(r'\\[\\d+\\]', '', text.text)\n",
        "\n",
        "    for para in root.findall('.//para'):\n",
        "        for weight in para.findall('weight'):\n",
        "            para.remove(weight)\n",
        "    for se in root.findall('.//se'):\n",
        "        se.attrib.pop('variant_id', None)\n",
        "    for se in root.findall('.//se[@lang=\"fr\"]'):\n",
        "        se.attrib['lang'] = 'kk'\n",
        "\n",
        "    tree.write(output_file, encoding='utf-8', xml_declaration=True)"
      ],
      "metadata": {
        "id": "2otKIxI1Hos6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### сразу делаем конверт очищенного xml в json формата сайта"
      ],
      "metadata": {
        "id": "S1T5qvp0eCIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "\n",
        "def convert_xml_to_json(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        xml_string = file.read()\n",
        "\n",
        "    root = ET.fromstring(xml_string)\n",
        "    paragraphs = root.findall('.//para')\n",
        "\n",
        "    result = []\n",
        "    for para in paragraphs:\n",
        "        para_id = para.get('id')\n",
        "        se_elements = para.findall('se')\n",
        "\n",
        "        paragraph = {\n",
        "            'id': para_id,\n",
        "            'kk': se_elements[0].text,\n",
        "            'ru': se_elements[1].text\n",
        "        }\n",
        "        result.append(paragraph)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for item in result:\n",
        "            file.write(json.dumps(item, ensure_ascii=False) + '\\n')"
      ],
      "metadata": {
        "id": "W2g67LnMeJXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'aidar_precleaned.xml'\n",
        "output_file = 'aidar_precleaned.json'\n",
        "\n",
        "convert_xml_to_json(input_file, output_file)"
      ],
      "metadata": {
        "id": "eWnNnWRreLbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\"id\": \"0\", \"kk\": \"Ертеде бір ханнің үш қызы болыпты.\", \"kk_words\": [], \"ru\": \"В прежние времена у одного хана было три дочери.\", \"ru_words\": []}\n",
        "{\"id\": \"1\", \"kk\": \"Екі үлкен қызын ұзатқаннан кейін, хан қолында қалған кенже қызын күйеуге беріп, ойын-тойын жасайды.\", \"kk_words\": [], \"ru\": \"Выдав двух старших дочерей, хан нашел мужа и младшей дочери, которая была у него на руках, устроил той (пир) и игрища.\", \"ru_words\": []}"
      ],
      "metadata": {
        "id": "q4QhunTM0_Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функция для преобразования json в формат, подходящий для подключения Elasticsearch ###"
      ],
      "metadata": {
        "id": "PBGREteTTv19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza"
      ],
      "metadata": {
        "id": "VtvXT2d_mxfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import json\n",
        "\n",
        "stanza.download('kk')\n",
        "stanza.download('ru')\n",
        "\n",
        "nlp_kk = stanza.Pipeline('kk', processors='tokenize,pos,lemma')\n",
        "nlp_ru = stanza.Pipeline('ru', processors='tokenize,pos,lemma')"
      ],
      "metadata": {
        "id": "DHXVKAcH8ChW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для добавления морфологического анализа к предложению на заданном языке\n",
        "def add_morphology(text, nlp, lang):\n",
        "    doc = nlp(text)\n",
        "    words = []\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            if word.upos == \"PUNCT\":\n",
        "                word_data = {\n",
        "                    \"wf\": word.text,\n",
        "                    \"wtype\": \"punct\"\n",
        "                }\n",
        "            else:\n",
        "                word_data = {\n",
        "                    \"wf\": word.text,\n",
        "                    \"wtype\": \"word\",\n",
        "                    \"lex\": word.lemma,\n",
        "                    \"gr.pos\": word.upos,\n",
        "                    \"gr.feats\": word.feats\n",
        "                }\n",
        "            words.append(word_data)\n",
        "    return words\n",
        "\n",
        "input_file = 'aidar_precleaned.json'\n",
        "\n",
        "output_file = 'aidar_output.json'\n",
        "\n",
        "with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
        "    for line in f_in:\n",
        "        data = json.loads(line)\n",
        "        \n",
        "        kk_text = data['kk']\n",
        "        ru_text = data['ru']\n",
        "        \n",
        "        kk_words = add_morphology(kk_text, nlp_kk, 'kk')\n",
        "        ru_words = add_morphology(ru_text, nlp_ru, 'ru')\n",
        "        \n",
        "        data['kk_words'] = kk_words\n",
        "        data['ru_words'] = ru_words\n",
        "        \n",
        "        output_string = json.dumps(data, ensure_ascii=False)\n",
        "        \n",
        "        f_out.write(output_string + '\\n')\n",
        "\n",
        "print(\"Обработка файла завершена.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mdhlvMB4vwz",
        "outputId": "b543b0ae-4384-4265-9d48-b1c57d33f84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработка файла завершена.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'Abai-zholy-1-2.xml'\n",
        "preprocess_xml(input_file)   "
      ],
      "metadata": {
        "id": "PKD05OecHvFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'Abai-zholy-3-4.xml'\n",
        "preprocess_xml(input_file)"
      ],
      "metadata": {
        "id": "6DWuRMu-Hyz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'adam_balyq.xml'\n",
        "preprocess_xml(input_file)   "
      ],
      "metadata": {
        "id": "77WFG1WrH7nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'koshpendiler-1.xml'\n",
        "preprocess_xml(input_file)"
      ],
      "metadata": {
        "id": "ofGwewMnIG6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'koshpendiler-2.xml'\n",
        "preprocess_xml(input_file)"
      ],
      "metadata": {
        "id": "AIHofDIvILVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'koshpendiler-3.xml'\n",
        "preprocess_xml(input_file)"
      ],
      "metadata": {
        "id": "ZITZx12NIRVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'lisa_i_volk.xml'\n",
        "preprocess_xml(input_file)"
      ],
      "metadata": {
        "id": "d7S5LIXIITdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6RDrvFHDwRx"
      },
      "source": [
        "## 1. Морфологическая разметка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSqlXOq_GFQh"
      },
      "source": [
        "### Смотрю на библиотеку kaznlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0iV-qDQGFQi"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "\n",
        "from kaznlp.tokenization.tokrex import TokenizeRex\n",
        "from kaznlp.tokenization.tokhmm import TokenizerHMM\n",
        "\n",
        "from kaznlp.morphology.analyzers import AnalyzerDD\n",
        "from kaznlp.morphology.taggers import TaggerHMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKAg_9wzGFQi",
        "outputId": "4a1f9fc8-8fdd-4a21-f046-4224ee1242a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Еңбек', 'етсең', 'ерінбей', ',', 'тояды', 'қарның', 'тіленбей', '.']]\n"
          ]
        }
      ],
      "source": [
        "# ==============\n",
        "# TOKENIZATION =\n",
        "# ==============\n",
        "\n",
        "\n",
        "mdl = os.path.join('kaznlp', 'tokenization', 'tokhmm.mdl')\n",
        "tokhmm = TokenizerHMM(model=mdl)\n",
        "sents_toks = tokhmm.tokenize(txt)\n",
        "print(sents_toks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "gcx-ssuaGFQk",
        "outputId": "67992c9e-cb5c-4281-8ee8-5570c0573cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\"алмасын\" is covered by the analyzer.\n",
            "Analyses are:\n",
            " 1) алма_R_ZE сы_S3 н_C4\n",
            " 2) ал_R_ET ма_ET_ETB с_ETB_ESM ы_S3 н_C4\n",
            " 3) ал_R_ET ма_ET_ETB сын_M2\n"
          ]
        }
      ],
      "source": [
        "# ============\n",
        "# MORPHOLOGY =\n",
        "# ============\n",
        "\n",
        "# create a morphological analyzer instance\n",
        "analyzer = AnalyzerDD()\n",
        "analyzer.load_model(os.path.join('kaznlp', 'morphology', 'mdl'))\n",
        "\n",
        "# try analysis\n",
        "print()\n",
        "wrd = 'алмасын'\n",
        "[iscovered, alist] = analyzer.analyze(wrd)\n",
        "print('\"{}\" is covered by the analyzer.'.format(wrd))\n",
        "print('Analyses are:')\n",
        "for i, a in enumerate(alist):\n",
        "    print(f'{str(i+1).rjust(2)}) {a}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03lYa4XWGFQl"
      },
      "source": [
        "**3 варианта разбора одного слова с непонятными обозначения гр. значений -> непонятно и неудобно..**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "ubb3u91kLfBo",
        "outputId": "f82aaa63-c3c8-4774-bdad-cfb56a75eebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "input sentence:\n",
            "['Еңбек', 'етсең', 'ерінбей', ',', 'тояды', 'қарның', 'тіленбей', '.']\n",
            "\n",
            "tagged sentence:\n",
            " 1) Еңбек          еңбек_R_ZE\n",
            " 2) етсең          ет_R_ET се_M4 ң_P2\n",
            " 3) ерінбей        ерінбей_R_X\n",
            " 4) ,              ,_R_UTR\n",
            " 5) тояды          то_R_ET я_T1 ды_P3\n",
            " 6) қарның         қар_R_ZE ның_C2\n",
            " 7) тіленбей       тіленбей_R_X\n",
            " 8) .              ._R_NKT\n"
          ]
        }
      ],
      "source": [
        "tagger = TaggerHMM(lyzer=analyzer)\n",
        "tagger.load_model(os.path.join('kaznlp', 'morphology', 'mdl'))\n",
        "\n",
        "txt = u'Еңбек етсең ерінбей, тояды қарның тіленбей.'\n",
        "tokenizer = TokenizerHMM(model=mdl)\n",
        "for sentence in tokenizer.tokenize(txt):\n",
        "    print(f'input sentence:\\n{sentence}\\n')\n",
        "    print('tagged sentence:')\n",
        "    lower_sentence = map(lambda x: x.lower(), sentence)\n",
        "    for i, a in enumerate(tagger.tag_sentence(lower_sentence)):\n",
        "        print(f'{str(i+1).rjust(2)}) {sentence[i].ljust(15)}{a}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "remIVhaPGFQl"
      },
      "source": [
        "## Функция для разметки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p97IhVM5GFQm"
      },
      "source": [
        "### Используем проверенную библиотеку *stanza*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TAscTOTVGFQm",
        "outputId": "4b098299-dcb4-4117-a6c2-ac4da28f9b21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanza in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.5.0)\n",
            "Requirement already satisfied: protobuf in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from stanza) (3.19.1)\n",
            "Requirement already satisfied: emoji in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from stanza) (1.7.0)\n",
            "Requirement already satisfied: six in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from stanza) (1.11.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from stanza) (4.62.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from stanza) (1.24.2)\n",
            "Requirement already satisfied: requests in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from stanza) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->stanza) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->stanza) (1.26.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tqdm->stanza) (0.4.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AKwzxW7GGFQm",
        "outputId": "523a3edb-f182-4b55-aa00-05d908568d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (4.6.4)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\users\\varva\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip3 install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "referenced_widgets": [
            "5cbde345c3bf4ad4b864828b4ef87ebb",
            "18e742ce26ed4406a67ed10e0c8d59f8"
          ]
        },
        "id": "B9pRVcrlGFQm",
        "outputId": "b72f0720-8104-401f-d8a8-0cd08d865fba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cbde345c3bf4ad4b864828b4ef87ebb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-17 18:25:15 INFO: Downloading default packages for language: kk (Kazakh) ...\n",
            "2023-05-17 18:25:17 INFO: File exists: C:\\Users\\varva\\stanza_resources\\kk\\default.zip\n",
            "2023-05-17 18:25:22 INFO: Finished downloading models and saved to C:\\Users\\varva\\stanza_resources.\n",
            "2023-05-17 18:25:22 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18e742ce26ed4406a67ed10e0c8d59f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-17 18:25:24 INFO: Loading these models for language: kk (Kazakh):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ktb     |\n",
            "| mwt       | ktb     |\n",
            "| pos       | ktb     |\n",
            "| lemma     | ktb     |\n",
            "| depparse  | ktb     |\n",
            "| ner       | kazNERD |\n",
            "=======================\n",
            "\n",
            "2023-05-17 18:25:24 INFO: Using device: cpu\n",
            "2023-05-17 18:25:24 INFO: Loading: tokenize\n",
            "2023-05-17 18:25:24 INFO: Loading: mwt\n",
            "2023-05-17 18:25:24 INFO: Loading: pos\n",
            "2023-05-17 18:25:25 INFO: Loading: lemma\n",
            "2023-05-17 18:25:25 INFO: Loading: depparse\n",
            "2023-05-17 18:25:25 INFO: Loading: ner\n",
            "2023-05-17 18:25:26 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "stanza.download('kk')\n",
        "pipeline = stanza.Pipeline(lang='kk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCKE6hbLGFQn"
      },
      "outputs": [],
      "source": [
        "pos_from_ud_to_gr_table = {\n",
        "            \"ADJ\": \"A\",\n",
        "            \"ADP\": \"PR\",\n",
        "            \"ADV\": \"ADV\",\n",
        "            \"AUX\": \"V, aux\",  \n",
        "            \"CCONJ\": \"CONJ\",\n",
        "            \"DET\": \"DET\",\n",
        "            \"INTJ\": \"INTJ\",\n",
        "            \"NOUN\": \"S\",\n",
        "            \"NUM\": \"NUM\",\n",
        "            \"PART\": \"PART\",\n",
        "            \"PRON\": \"PRO\",\n",
        "            \"PROPN\": \"S, propn\", \n",
        "            \"SCONJ\": \"CONJ\",\n",
        "            \"VERB\": \"V\",\n",
        "        }\n",
        "\n",
        "feats_from_ud_to_gr_table = {\n",
        "            \"Gender\": {\n",
        "                \"Fem\":\"f\",\n",
        "                \"Masc\":\"m\"\n",
        "            },\n",
        "            \"AdpType\": {\n",
        "                \"POST\":\"POSL\",\n",
        "            },\n",
        "            \"Number\": {\n",
        "                \"Plur\":\"pl\",\n",
        "                \"Sing\":\"sg\"\n",
        "            },\n",
        "            \"Case\": {\n",
        "                \"Acc\":\"acc\", \n",
        "                \"Dat\":\"dat\", \n",
        "                \"Erg\":\"erg\", \n",
        "                \"Gen\":\"gen\", \n",
        "                \"Ine\":\"voc\", \n",
        "                \"Ins\":\"ins\", \n",
        "                \"Nom\":\"nom\"\n",
        "            },\n",
        "            \"VerbForm\": {\n",
        "                \"Conv\":\"ger\", \n",
        "                \"Fin\":\"fin\", \n",
        "                \"Inf\":\"inf\", \n",
        "                \"Part\":\"partcp\"\n",
        "            },\n",
        "            \"Mood\": {\n",
        "                \"Imp\":\"imper\", \n",
        "                \"Ind\":\"indic\", \n",
        "                \"Sub\":\"sub\"\n",
        "            },\n",
        "            \"Tense\": {\n",
        "                \"Fut\":\"fut\", \n",
        "                \"Past\":\"praet\", \n",
        "                \"Pres\":\"praes\"\n",
        "            },\n",
        "            \"Voice\": {\n",
        "                \"Act\":\"act\", \n",
        "                \"Pass\":\"pass\"\n",
        "            },\n",
        "            \"Person\": {\n",
        "                \"1\":\"1p\", \n",
        "                \"2\":\"2p\", \n",
        "                \"3\":\"3p\"\n",
        "            }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDCl-gSNGFQn"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "class XmlProcessor:\n",
        "    def __init__(self, lang):\n",
        "        stanza.download(lang)\n",
        "        self.pipeline = stanza.Pipeline(lang='kk')\n",
        "\n",
        "    def parse_ud_feats(self, token):\n",
        "        dummy = dict()\n",
        "        ud = [token.upos, dummy]\n",
        "        if token.feats is None:\n",
        "            return ud\n",
        "        feats = dict()\n",
        "        raw_feats = token.feats.split('|')\n",
        "        for raw_feat in raw_feats:\n",
        "            split = raw_feat.split('=')\n",
        "            feats[split[0]] = split[1]\n",
        "        ud[1] = feats\n",
        "        return ud\n",
        "\n",
        "    def from_ud_to_gr(self, ud_feats):\n",
        "        gr_feats = []\n",
        "        if len(ud_feats) == 0:\n",
        "            return \"\"\n",
        "        if ud_feats[0] in pos_from_ud_to_gr_table:\n",
        "            gr_feats.append(pos_from_ud_to_gr_table[ud_feats[0]])\n",
        "        \n",
        "        for key, value in ud_feats[1].items():\n",
        "            if key in feats_from_ud_to_gr_table:\n",
        "                if value in feats_from_ud_to_gr_table[key]:\n",
        "                    gr_feats.append(feats_from_ud_to_gr_table[key][value])\n",
        "        return ','.join(gr_feats)\n",
        "\n",
        "    def process_xml(self, input_file, output_file):\n",
        "        tree = ET.parse(input_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        for para in root.iter('para'):\n",
        "            for se in para.iter('se'):\n",
        "                if se.get('lang') == 'kk':\n",
        "                    text = se.text.strip()\n",
        "                    doc = self.pipeline(text)\n",
        "                    se.text = ''\n",
        "                    for sent in doc.sentences:\n",
        "                        for token in sent.tokens:\n",
        "                            word_element = ET.Element('w')\n",
        "                            for word in token.words:\n",
        "                                ana_element = ET.Element('ana')\n",
        "                                ana_element.set('lex', word.lemma)\n",
        "                                ud_feats = self.parse_ud_feats(word)\n",
        "                                ana_element.set('gr', self.from_ud_to_gr(ud_feats))\n",
        "                                word_text_element = ET.Element('text')\n",
        "                                word_text_element.text = token.text\n",
        "                                word_element.append(ana_element)\n",
        "                                word_element.append(word_text_element)\n",
        "                            se.append(word_element)\n",
        "\n",
        "        tree.write(output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "referenced_widgets": [
            "f8deed80d02c4edcb4050eb3a7af503c",
            "2ef1375d8fb04a2a84852a1bd9d4adf9"
          ]
        },
        "id": "qvczZq6tGFQn",
        "outputId": "a0f38386-7556-4229-f907-33dfefdad623"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8deed80d02c4edcb4050eb3a7af503c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-17 18:26:22 INFO: Downloading default packages for language: kk (Kazakh) ...\n",
            "2023-05-17 18:26:23 INFO: File exists: C:\\Users\\varva\\stanza_resources\\kk\\default.zip\n",
            "2023-05-17 18:26:27 INFO: Finished downloading models and saved to C:\\Users\\varva\\stanza_resources.\n",
            "2023-05-17 18:26:27 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ef1375d8fb04a2a84852a1bd9d4adf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-17 18:26:29 INFO: Loading these models for language: kk (Kazakh):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ktb     |\n",
            "| mwt       | ktb     |\n",
            "| pos       | ktb     |\n",
            "| lemma     | ktb     |\n",
            "| depparse  | ktb     |\n",
            "| ner       | kazNERD |\n",
            "=======================\n",
            "\n",
            "2023-05-17 18:26:29 INFO: Using device: cpu\n",
            "2023-05-17 18:26:29 INFO: Loading: tokenize\n",
            "2023-05-17 18:26:29 INFO: Loading: mwt\n",
            "2023-05-17 18:26:29 INFO: Loading: pos\n",
            "2023-05-17 18:26:30 INFO: Loading: lemma\n",
            "2023-05-17 18:26:30 INFO: Loading: depparse\n",
            "2023-05-17 18:26:30 INFO: Loading: ner\n",
            "2023-05-17 18:26:31 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "xml_processor = XmlProcessor('kk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgN3O8V0GFQo"
      },
      "outputs": [],
      "source": [
        "processor.process_xml('drakon.xml', 'drakon_processed.xml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSNwssrNGFQo"
      },
      "source": [
        "## 2. Посточистка файла"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hib-Wok1GFQo"
      },
      "source": [
        "остается:\n",
        "- убрать разметку со знаков препинания\n",
        "- вставить пробелы между тэгами </w\\><w\n",
        "- убрать тег <text\\>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rrfsVQzGFQo"
      },
      "outputs": [],
      "source": [
        "with open(\"aidar_processed.xml\", 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "    with open(\"aidar_nkrya.xml\", 'w', encoding='utf-8') as f_perf:\n",
        "        for line in lines:\n",
        "            line = re.sub('</w><w', '</w> <w', line)\n",
        "            line = re.sub('<text>', '', line)\n",
        "            line = re.sub('</text>', '', line)\n",
        "            line = re.sub('<w><ana lex=\"«\" gr=\"\" />«</w> ', '«', line)\n",
        "            line = re.sub(' <w><ana lex=\"»\" gr=\"\" />»</w>', '»', line)\n",
        "            line = re.sub(' <w><ana lex=\"...\" gr=\"\" />...</w>', '...', line)\n",
        "            line = re.sub(' <w><ana lex=\".\" gr=\"\" />.</w>', '.', line)\n",
        "            line = re.sub(' <w><ana lex=\",\" gr=\"\" />,</w>', ',', line)\n",
        "            line = re.sub(' <w><ana lex=\"!\" gr=\"\" />!</w>', '!', line)\n",
        "            line = re.sub(' <w><ana lex=\"?\" gr=\"\" />?</w>', '?', line)\n",
        "            line = re.sub(' <w><ana lex=\":\" gr=\"\" />:</w>', ':', line)\n",
        "            line = re.sub(' <w><ana lex=\";\" gr=\"\" />;</w>', ';', line)\n",
        "            line = re.sub('<w><ana lex=\"—\" gr=\"\" />—</w>', '—', line)\n",
        "            line = re.sub(' <w><ana lex=\"..\" gr=\"\" />..</w>', '..', line)\n",
        "            f_perf.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmt5i1fzGFQp"
      },
      "source": [
        "# II этап: подготовка текстов для Tsacorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE9PTqPcGFQp"
      },
      "source": [
        "### Как выглядит json-файл для Цакорпуса:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCxB0Lo4GFQp"
      },
      "source": [
        "**База:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFenk_ZxGFQp"
      },
      "outputs": [],
      "source": [
        "{\n",
        "    \"meta\": {\n",
        "        \"title\": \"...\",\n",
        "        \"author\": \"...\",\n",
        "        \"year\": \"...\",\n",
        "        \"translator\": \"...\",\n",
        "    },\n",
        "    \"sentences\": [\n",
        "        {\n",
        "            \"text\": \"...\",\n",
        "            \"words\": [\n",
        "            ],\n",
        "            \"lang\": 0,\n",
        "            \"para_alignment\": {\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"...\",\n",
        "            \"words\": [\n",
        "            ],\n",
        "            \"lang\": 1,\n",
        "            \"para_alignment\": {\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHYh_WF1GFQq"
      },
      "source": [
        "**\"words\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZraIKIJGFQq"
      },
      "outputs": [],
      "source": [
        "\"words\": [\n",
        "    {\n",
        "        \"wf\": \"...\",\n",
        "        \"wtype\": \"word/punct\",   # word - если слово, punct - если пунктуация \n",
        "        \"ana\": [\n",
        "            \"lex\": \"...\",        # лемма\n",
        "            \"gr.pos\": \"...\",     # часть речи\n",
        "            \"gr.number\": \"...\",  # число, если есть\n",
        "            \"gr.case\": \"...\"     # падеж, если есть\n",
        "        ],\n",
        "        \"sentence_index\": 0,     # индекс слова в предложении\n",
        "        \"off_start\": 0,          # индекс первой буквы словоформы в полном предложении\n",
        "        \"off_end\": 0,            # индекс последней буквы словоформы в полном предложении\n",
        "        \"next_word\": 1,          # индекс следующего слова в предложении\n",
        "        \"sentence_index_neg\": 5, # количество слов до конца предложения\n",
        "    },\n",
        "    {\n",
        "        \"wf\": \"...\",\n",
        "        \"wtype\": \"word/punct\",\n",
        "        \"ana\": [\n",
        "            \"lex\": \"...\",\n",
        "            \"gr.pos\": \"...\",\n",
        "            \"gr.number\": \"...\",\n",
        "            \"gr.case\": \"...\"\n",
        "        ],\n",
        "        \"sentence_index\": 0,    \n",
        "        \"off_start\": 0,        \n",
        "        \"off_end\": 0,\n",
        "        \"next_word\": 1,\n",
        "        \"sentence_index_neg\": 5,\n",
        "    },\n",
        "    ...\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPqwfWxfGFQq"
      },
      "source": [
        "### Функция для преобразования XML в JSON без разметки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuPgtdfdGFQq"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "\n",
        "def xml_to_json(file_path, output_file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    sentences = []\n",
        "\n",
        "    off_start_ru = 0\n",
        "    off_start_kk = 0\n",
        "\n",
        "    for para in root.iter('para'):\n",
        "        para_id = para.attrib['id']\n",
        "        for se in para.iter('se'):\n",
        "            lang = se.attrib['lang']\n",
        "            text = se.text\n",
        "\n",
        "            if text is not None:\n",
        "                text = text.strip()\n",
        "                words = []\n",
        "\n",
        "                if lang == 'ru':\n",
        "                    off_start = off_start_ru\n",
        "                    off_start_ru += len(text) + 2\n",
        "                else:\n",
        "                    off_start = off_start_kk\n",
        "                    off_start_kk += len(text) + 2\n",
        "\n",
        "                sentence = {\n",
        "                    'text': text,\n",
        "                    'words': words,\n",
        "                    'lang': lang,\n",
        "                    'para_alignment': {\n",
        "                        'off_start': off_start,\n",
        "                        'off_end': off_start + len(text),\n",
        "                        'para_id': para_id\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                sentences.append(sentence)\n",
        "\n",
        "    json_data = {'sentences': sentences}\n",
        "\n",
        "    with open(output_file_path, 'w', encoding=\"utf-8\") as output_file:\n",
        "        json.dump(json_data, output_file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgoLaGPBGFQr",
        "outputId": "3f5a329d-9fcd-4df4-d90a-93086f6d7b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Преобразование XML в JSON завершено. Разультат сохранен в файл suka.json\n"
          ]
        }
      ],
      "source": [
        "xml_file_path = 'drakon_postcleaned.xml'\n",
        "output_file_path = 'drakon_tsakorpus.json'\n",
        "xml_to_json(xml_file_path, output_file_path)\n",
        "print(\"Преобразование XML в JSON завершено. Разультат сохранен в файл\", output_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YuBXfruGFQr"
      },
      "source": [
        "**Далее конвертируем файл в utf-8 в конвертере**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maaxdIlpGFQr"
      },
      "source": [
        "### Добавляем морфологическую разметку"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-Y4yuC8GFQr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import stanza\n",
        "\n",
        "nlp_kk = stanza.Pipeline('kk', processors='tokenize,mwt,pos,lemma')\n",
        "nlp_ru = stanza.Pipeline('ru', processors='tokenize,pos,lemma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLoLiI-CGFQs"
      },
      "outputs": [],
      "source": [
        "def add_morphology(data):\n",
        "\n",
        "    for sentence in data['sentences']:\n",
        "        lang = sentence['lang']\n",
        "        text = sentence['text']\n",
        "\n",
        "        if lang == 'kk':\n",
        "            nlp = nlp_kk\n",
        "        elif lang == 'ru':\n",
        "            nlp = nlp_ru\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        doc = nlp(text)\n",
        "\n",
        "        words = []\n",
        "        word_index = 0\n",
        "        for sent in doc.sentences:\n",
        "            for word in sent.words:\n",
        "                feats = word.feats\n",
        "                case_value = None\n",
        "                number_value = None\n",
        "                if feats is not None:\n",
        "                    feats_list = feats.split('|')\n",
        "                    for feat in feats_list:\n",
        "                        if feat.startswith('Case='):\n",
        "                            case_value = feat.split('=')[1]\n",
        "                        elif feat.startswith('Number='):\n",
        "                            number_value = feat.split('=')[1]\n",
        "                            break\n",
        "                        else:\n",
        "                            case_value = None\n",
        "                else:\n",
        "                    case_value = None\n",
        "                \n",
        "                token = {\n",
        "                    'wf': word.text,\n",
        "                    'wtype': 'word' if word.upos != 'PUNCT' else 'punct',\n",
        "                    'ana': [\n",
        "                        {\n",
        "                            'lex': word.lemma,\n",
        "                            'gr.pos': word.upos,\n",
        "                            'gr.number': number_value,\n",
        "                            'gr.case': case_value,\n",
        "                        }\n",
        "                    ],\n",
        "                    'sentence_index': word_index,\n",
        "                    'off_start': word.start_char,\n",
        "                    'off_end': word.end_char - 1 if word.end_char is not None else None,\n",
        "                    'next_word': word_index + 1,\n",
        "                    'sentence_index_neg': len(sent.words) - word_index - 1\n",
        "                }\n",
        "                words.append(token)\n",
        "                word_index += 1\n",
        "\n",
        "        sentence['words'] = words\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R20YQJwyGFQt",
        "outputId": "0e695da7-d6ce-461d-dd6f-649c2e6d8a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Морфологическая разметка добавлена и сохранена в файл: aidar_tsacorpus.json\n"
          ]
        }
      ],
      "source": [
        "json_file_path = 'drakon_json.json'\n",
        "output_file_path = 'drakon_tsakorpus.json'\n",
        "\n",
        "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "data_with_morphology = add_morphology(data)\n",
        "\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    json.dump(data_with_morphology, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Морфологическая разметка добавлена и сохранена в файл:\", output_file_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}